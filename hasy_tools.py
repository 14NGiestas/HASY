#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Tools for the HASY dataset.

Type `./hasy_tools.py --help` for the command line tools and `help(hasy_tools)`
in the interactive Python shell for the module options of hasy_tools.
"""

import logging
import csv
import os
import random
from PIL import Image, ImageDraw
import sys

from six.moves import cPickle as pickle
import numpy as np
import scipy.ndimage
import matplotlib.pyplot as plt

logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                    level=logging.INFO,
                    stream=sys.stdout)

__version__ = "v1.0"


def _load_csv(filepath, delimiter=',', quotechar="'"):
    """
    Load a CSV file.

    Parameters
    ----------
    filepath : str
        Path to a CSV file
    delimiter : str, optional
    quotechar : str, optional

    Returns
    -------
    list of dicts : Each line of the CSV file is one element of the list.
    """
    data = []
    with open(filepath, 'rb') as csvfile:
        reader = csv.DictReader(csvfile,
                                delimiter=delimiter,
                                quotechar=quotechar)
        for row in reader:
            data.append(row)
    return data


def generate_index(dataset_path):
    """
    Generate an index 0...k for the k labels.

    Parameters
    ----------
    dataset_path : str
        Path to the main directory HASYv1 which contains hasy-test-labels.csv

    Returns
    -------
    dict : Maps a symbol_id as in hasy-test-labels.csv and
        hasy-train-labels.csv to an integer in 0...k, where k is the total
        number of unique labels.
    """
    symbol_id2index = {}
    data = _load_csv(os.path.join(dataset_path, 'hasy-test-labels.csv'))
    i = 0
    for item in data:
        if item['symbol_id'] not in symbol_id2index:
            symbol_id2index[item['symbol_id']] = i
            i += 1
    return symbol_id2index


def load_images(dataset_path, csv_file_path, symbol_id2index, one_hot=True):
    """
    Load the images into a 4D uint8 numpy array [index, y, x, depth].

    This also pickles the loaded and parsed data to improve the speed of
    subsequent runs.

    Parameters
    ----------
    dataset_path : str
        The path to the main directory HASY_v1
    csv_file_path : str
        'hasy-test-labels.csv' or 'hasy-train-labels.csv'
    symbol_id2index : dict
        Dictionary generated by generate_index

    Returns
    -------
    images, labels : Images is a 4D uint8 numpy array [index, y, x, depth]
                     and labels is a 2D uint8 numpy array [index][1-hot enc].
    """
    WIDTH, HEIGHT = 32, 32
    csv_filepath = os.path.join(dataset_path, csv_file_path)
    pickle_filepath = csv_filepath + ".pickle"
    if os.path.isfile(pickle_filepath):
        with open(pickle_filepath, 'rb') as handle:
            data = pickle.load(handle)
    else:
        data = _load_csv(csv_filepath)
        images = np.zeros((len(data), WIDTH, HEIGHT, 1))
        labels = []
        for i, data_item in enumerate(data):
            fname = os.path.join(dataset_path, data_item['path'])
            images[i, :, :, 0] = scipy.ndimage.imread(fname,
                                                      flatten=False,
                                                      mode='L')
            label = symbol_id2index[data_item['symbol_id']]
            labels.append(label)
        # Pickle it to speed up later runs
        data = images, np.array(labels)
        with open(pickle_filepath, 'wb') as handle:
            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
    if one_hot:
        data = (data[0], np.eye(len(symbol_id2index))[data[1]])
    return data


def _is_valid_png(filepath):
    """
    Check if the PNG image is valid.

    Parameters
    ----------
    filepath : str
        Path to a PNG image

    Returns
    -------
    bool : True if the PNG image is valid, otherwise False
    """
    try:
        test = Image.open(filepath)
        test.close()
        return True
    except:
        return False


def _verify_all():
    """Verify all PNG files in the training and test directories."""
    for csv_data_path in ['hasy-test-labels.csv', 'hasy-train-labels.csv']:
        train_data = _load_csv(csv_data_path)
        for data_item in train_data:
            if not _is_valid_png(data_item['path']):
                logging.info("%s is invalid." % data_item['path'])
        logging.info("Checked %i items of %s." %
                     (len(train_data), csv_data_path))


def create_random_overview(img_src, x_images, y_images):
    """Create a random overview of images."""
    # Create canvas
    background = Image.new('RGB',
                           (35 * x_images, 35 * y_images),
                           (255, 255, 255))
    bg_w, bg_h = background.size
    # Paste image on canvas
    for x in range(x_images):
        for y in range(y_images):
            path = random.choice(img_src)['path']
            img = Image.open(path, 'r')
            img_w, img_h = img.size
            offset = (35 * x, 35 * y)
            background.paste(img, offset)
    # Draw lines
    draw = ImageDraw.Draw(background)
    for y in range(y_images):  # horizontal lines
        draw.line((0, 35 * y - 2, 35 * x_images, 35 * y - 2), fill=0)
    for x in range(x_images):  # vertical lines
        draw.line((35 * x - 2, 0, 35 * x - 2, 35 * y_images), fill=0)
    # Store
    background.save('hasy-overview.png')


def _get_colors(data, verbose=False):
    """
    Get how often each color is used in data.

    Parameters
    ----------
    data : dict
        with key 'path' pointing to an image

    Returns
    -------
    color_count : dict
        Maps a grayscale value (0..255) to how often it was in `data`
    """
    color_count = {}
    for i in range(256):
        color_count[i] = 0
    for i, data_item in enumerate(data):
        if i % 1000 == 0 and i > 0 and verbose:
            print("%i of %i done" % (i, len(data)))
        fname = os.path.join('.', data_item['path'])
        img = scipy.ndimage.imread(fname, flatten=False, mode='L')
        for row in img:
            for pixel in row:
                color_count[pixel] += 1
    return color_count


def data_by_class(data):
    """
    Organize `data` by class.

    Parameters
    ----------
    data : list of dicts
        Each dict contains the key `symbol_id` which is the class label.

    Returns
    -------
    dbc : dict
        mapping class labels to lists of dicts
    """
    dbc = {}
    for item in data:
        if item['symbol_id'] in dbc:
            dbc[item['symbol_id']].append(item)
        else:
            dbc[item['symbol_id']] = [item]
    return dbc


def _get_color_statistics(csv_filepath='hasy-train-labels.csv', verbose=False):
    symbolid2latex = _get_symbolid2latex()
    data = _load_csv(csv_filepath)
    black_level, classes = [], []
    for symbol_id, elements in data_by_class(data).items():
        colors = _get_colors(elements)
        b = colors[0]
        w = colors[255]
        black_level.append(float(b) / (b + w))
        classes.append(symbol_id)
        if verbose:
            print("%s:\t%0.4f" % (symbol_id, black_level[-1]))
    print("Average black level: %0.4f" % np.average(black_level))
    print("Median black level: %0.4f" % np.median(black_level))
    print("Minimum black level: %0.4f (class: %s)" %
          (min(black_level),
           [symbolid2latex[c]
            for bl, c in zip(black_level, classes) if bl <= min(black_level)]))
    print("Maximum black level: %0.4f (class: %s)" %
          (max(black_level),
           [symbolid2latex[c]
            for bl, c in zip(black_level, classes) if bl >= max(black_level)]))


def _get_symbolid2latex(csv_filepath='symbols.csv'):
    symbol_data = _load_csv(csv_filepath)
    symbolid2latex = {}
    for row in symbol_data:
        symbolid2latex[row['symbol_id']] = row['latex']
    return symbolid2latex


def _analyze_class_distribution(dataset_path='.',
                                csv_filepath='hasy-train-labels.csv',
                                max_data=1000,
                                bin_size=25):
    """Plot the distribution of training data over graphs."""
    symbol_id2index = generate_index(dataset_path)
    index2symbol_id = {}
    for index, symbol_id in symbol_id2index.items():
        index2symbol_id[symbol_id] = index
    data, y = load_images(dataset_path, csv_filepath, symbol_id2index,
                          one_hot=False)

    data = {}
    for el in y:
        if el in data:
            data[el] += 1
        else:
            data[el] = 1
    classes = data
    images = len(y)

    # Create plot
    print("Classes: %i" % len(classes))
    print("Images: %i" % images)

    class_counts = sorted([count for _, count in classes.items()])
    print("\tmin: %i" % min(class_counts))
    # plt.title('HASY training data distribution')
    plt.xlabel('Amount of available training images')
    plt.ylabel('Number of classes')
    min_examples = 0
    plt.hist(class_counts, bins=range(min_examples, max_data + 1, bin_size))
    # plt.show()
    filename = '{}.pdf'.format('training-data-dist')
    plt.savefig(filename)
    logging.info("Plot has been saved as {}".format(filename))

    symbolid2latex = _get_symbolid2latex()

    top10 = sorted(classes.items(), key=lambda n: n[1], reverse=True)[:10]
    top10_data = 0
    for index, count in top10:
        print("\t%s:\t%i" % (symbolid2latex[index2symbol_id[index]], count))
        top10_data += count
    total_data = sum([count for index, count in classes.items()])
    print("Top-10 has %i training data (%0.2f%% of total)" %
          (top10_data, float(top10_data) * 100.0 / total_data))
    print("%i classes have more than %i data items." %
          (sum([1 for _, count in classes.items() if count > max_data]),
           max_data))


def _analyze_pca(dataset_path='.', csv_filepath='hasy-train-labels.csv'):
    from sklearn.decomposition import PCA
    import itertools as it

    symbol_id2index = generate_index(dataset_path)
    data, y = load_images(dataset_path,
                          csv_filepath,
                          symbol_id2index,
                          one_hot=False)
    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])
    pca = PCA()
    pca.fit(data)
    sum_ = 0.0
    done_values = [None, None, None]
    done_points = [False, False, False]
    chck_points = [0.9, 0.95, 0.99]
    for counter, el in enumerate(pca.explained_variance_ratio_):
        sum_ += el
        for check_point, done, i in zip(chck_points, done_points, it.count()):
            if not done and sum_ >= check_point:
                done_points[i] = counter
                done_values[i] = sum_
    for components, variance in zip(done_points, done_values):
        print("%i components explain %0.2f of the variance" %
              (components, variance))


def _get_euclidean_dist(e1, e2):
    """Calculate the euclidean distance between e1 and e2."""
    e1 = e1.flatten()
    e2 = e2.flatten()
    return sum([(el1 - el2)**2 for el1, el2 in zip(e1, e2)])**0.5


def _inner_class_distance(data):
    distances = []
    mean_img = None
    for e1 in data:  # nicer?
        fname1 = os.path.join('.', e1['path'])
        img1 = scipy.ndimage.imread(fname1, flatten=False, mode='L')
        if mean_img is None:
            # print("done")
            mean_img = img1.tolist()
        else:
            # print("before")
            # print(mean_img)
            # print("add")
            # print(img1)
            mean_img += img1
            # print("after")
            # print(mean_img)
    mean_img = mean_img / float(len(data))
    # mean_img = thresholdize(mean_img, 'auto')
    scipy.misc.imshow(mean_img)
    for e1 in data:
        fname1 = os.path.join('.', e1['path'])
        img1 = scipy.ndimage.imread(fname1, flatten=False, mode='L')
        dist = _get_euclidean_dist(img1, mean_img)
        distances.append(dist)

    return (distances, mean_img)


def thresholdize(img, threshold=0.5):
    """Create a black-and-white image from a grayscale image."""
    img_new = []
    if threshold == 'auto':
        img_flat = sorted(img.flatten())
        threshold_ind = int(0.85 * len(img_flat))
        threshold = img_flat[threshold_ind]
    for row in img:
        bla = []
        for col in row:
            if col > threshold:
                bla.append(1)
            else:
                bla.append(0)
        img_new.append(bla)
    return np.array(img_new)


def _analyze_distances(dataset_path='.', csv_filepath='hasy-train-labels.csv'):
    """Analyze the distance between elements of one class and class means."""
    symbolid2latex = _get_symbolid2latex()
    data = _load_csv(csv_filepath)
    data = data_by_class(data)
    mean_imgs = []
    for class_, data_class in data.items():
        latex = symbolid2latex[class_]
        d, mean_img = _inner_class_distance(data_class)
        # scipy.misc.imshow(mean_img)
        print("%s: min=%0.4f, avg=%0.4f, median=%0.4f max=%0.4f" %
              (latex, np.min(d), np.average(d), np.median(d), np.max(d)))
        distarr = sorted([(label, mean_c, _get_euclidean_dist(mean_c, mean_img))
                          for label, mean_c in mean_imgs],
                         key=lambda n: n[2])
        for label, mean_c, d in distarr:
            print("\t%s: %0.4f" % (label, d))
        mean_imgs.append((latex, mean_img))


def _get_parser():
    """Get parser object for hasy_tools.py."""
    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    parser = ArgumentParser(description=__doc__,
                            formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument("--dataset",
                        dest="dataset",
                        default='hasy-train-labels.csv',
                        help="specify which data to use")
    parser.add_argument("--verify",
                        dest="verify",
                        action="store_true",
                        default=False,
                        help="verify PNG files")
    parser.add_argument("--overview",
                        dest="overview",
                        action="store_true",
                        default=False,
                        help="Get overview of data")
    parser.add_argument("--analyze_color",
                        dest="analyze_color",
                        action="store_true",
                        default=False,
                        help="Analyze the color distribution")
    parser.add_argument("--class_distribution",
                        dest="class_distribution",
                        action="store_true",
                        default=False,
                        help="Analyze the class distribution")
    parser.add_argument("--distances",
                        dest="distances",
                        action="store_true",
                        default=False,
                        help="Analyze the euclidean distance distribution")
    parser.add_argument("--pca",
                        dest="pca",
                        action="store_true",
                        default=False,
                        help=("Show how many principal components explain "
                              "90%% / 95%% / 99%% of the variance"))
    return parser


if __name__ == "__main__":
    args = _get_parser().parse_args()
    if args.verify:
        _verify_all()
    if args.overview:
        img_src = _load_csv(args.dataset)
        create_random_overview(img_src, x_images=10, y_images=10)
    if args.analyze_color:
        _get_color_statistics(csv_filepath=args.dataset)
    if args.class_distribution:
        _analyze_class_distribution(csv_filepath=args.dataset,
                                    max_data=200,
                                    bin_size=5)
    if args.pca:
        _analyze_pca(csv_filepath='hasy-train-labels.csv')
    if args.distances:
        _analyze_distances(csv_filepath='hasy-train-labels.csv')
