#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Tools for the HASY dataset.

Type `./hasy_tools.py --help` for the command line tools and `help(hasy_tools)`
in the interactive Python shell for the module options of hasy_tools.
"""

import logging
import csv
import os
import random
from PIL import Image, ImageDraw
import sys

from six.moves import cPickle as pickle
import numpy
import scipy.ndimage
import matplotlib.pyplot as plt

logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                    level=logging.INFO,
                    stream=sys.stdout)

__version__ = "v1.0"


def _load_csv(filepath, delimiter=',', quotechar="'"):
    """
    Load a CSV file.

    Parameters
    ----------
    filepath : str
        Path to a CSV file
    delimiter : str, optional
    quotechar : str, optional

    Returns
    -------
    list of dicts : Each line of the CSV file is one element of the list.
    """
    data = []
    with open(filepath, 'rb') as csvfile:
        reader = csv.DictReader(csvfile,
                                delimiter=delimiter,
                                quotechar=quotechar)
        for row in reader:
            data.append(row)
    return data


def generate_index(dataset_path):
    """
    Generate an index 0...k for the k labels.

    Parameters
    ----------
    dataset_path : str
        Path to the main directory HASYv1 which contains hasy-test-labels.csv

    Returns
    -------
    dict : Maps a symbol_id as in hasy-test-labels.csv and
        hasy-train-labels.csv to an integer in 0...k, where k is the total
        number of unique labels.
    """
    symbol_id2index = {}
    data = _load_csv(os.path.join(dataset_path, 'hasy-test-labels.csv'))
    i = 0
    for item in data:
        if item['symbol_id'] not in symbol_id2index:
            symbol_id2index[item['symbol_id']] = i
            i += 1
    return symbol_id2index


def load_images(dataset_path, csv_file_path, symbol_id2index, one_hot=True):
    """
    Load the images into a 4D uint8 numpy array [index, y, x, depth].

    This also pickles the loaded and parsed data to improve the speed of
    subsequent runs.

    Parameters
    ----------
    dataset_path : str
        The path to the main directory HASY_v1
    csv_file_path : str
        'hasy-test-labels.csv' or 'hasy-train-labels.csv'
    symbol_id2index : dict
        Dictionary generated by generate_index

    Returns
    -------
    images, labels : Images is a 4D uint8 numpy array [index, y, x, depth]
                     and labels is a 2D uint8 numpy array [index][1-hot enc].
    """
    WIDTH, HEIGHT = 32, 32
    csv_filepath = os.path.join(dataset_path, csv_file_path)
    pickle_filepath = csv_filepath + ".pickle"
    if os.path.isfile(pickle_filepath):
        with open(pickle_filepath, 'rb') as handle:
            data = pickle.load(handle)
    else:
        data = _load_csv(csv_filepath)
        images = numpy.zeros((len(data), WIDTH, HEIGHT, 1))
        labels = []
        for i, data_item in enumerate(data):
            fname = os.path.join(dataset_path, data_item['path'])
            images[i, :, :, 0] = scipy.ndimage.imread(fname,
                                                      flatten=False,
                                                      mode='L')
            label = symbol_id2index[data_item['symbol_id']]
            labels.append(label)
        # Pickle it to speed up later runs
        data = images, numpy.array(labels)
        with open(pickle_filepath, 'wb') as handle:
            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
    if one_hot:
        data = (data[0], numpy.eye(len(symbol_id2index))[data[1]])
    return data


def _is_valid_png(filepath):
    """
    Check if the PNG image is valid.

    Parameters
    ----------
    filepath : str
        Path to a PNG image

    Returns
    -------
    bool : True if the PNG image is valid, otherwise False
    """
    try:
        test = Image.open(filepath)
        test.close()
        return True
    except:
        return False


def _verify_all():
    """Verify all PNG files in the training and test directories."""
    for csv_data_path in ['hasy-test-labels.csv', 'hasy-train-labels.csv']:
        train_data = _load_csv(csv_data_path)
        for data_item in train_data:
            if not _is_valid_png(data_item['path']):
                logging.info("%s is invalid." % data_item['path'])
        logging.info("Checked %i items of %s." %
                     (len(train_data), csv_data_path))


def create_random_overview(img_src, x_images, y_images):
    """Create a random overview of images."""
    # Create canvas
    background = Image.new('RGB',
                           (35 * x_images, 35 * y_images),
                           (255, 255, 255))
    bg_w, bg_h = background.size
    # Paste image on canvas
    for x in range(x_images):
        for y in range(y_images):
            path = random.choice(img_src)['path']
            img = Image.open(path, 'r')
            img_w, img_h = img.size
            offset = (35 * x, 35 * y)
            background.paste(img, offset)
    # Draw lines
    draw = ImageDraw.Draw(background)
    for y in range(y_images):  # horizontal lines
        draw.line((0, 35 * y - 2, 35 * x_images, 35 * y - 2), fill=0)
    for x in range(x_images):  # vertical lines
        draw.line((35 * x - 2, 0, 35 * x - 2, 35 * y_images), fill=0)
    # Store
    background.save('hasy-overview.png')


def _get_colors(data, verbose=False):
    """
    Get how often each color is used in data.

    Parameters
    ----------
    data : dict
        with key 'path' pointing to an image

    Returns
    -------
    color_count : dict
        Maps a grayscale value (0..255) to how often it was in `data`
    """
    color_count = {}
    for i in range(256):
        color_count[i] = 0
    for i, data_item in enumerate(data):
        if i % 1000 == 0 and i > 0 and verbose:
            print("%i of %i done" % (i, len(data)))
        fname = os.path.join('.', data_item['path'])
        img = scipy.ndimage.imread(fname, flatten=False, mode='L')
        for row in img:
            for pixel in row:
                color_count[pixel] += 1
    return color_count


def data_by_class(data):
    """
    Organize `data` by class.

    Parameters
    ----------
    data : list of dicts
        Each dict contains the key `symbol_id` which is the class label.

    Returns
    -------
    dbc : dict
        mapping class labels to lists of dicts
    """
    dbc = {}
    for item in data:
        if item['symbol_id'] in dbc:
            dbc[item['symbol_id']].append(item)
        else:
            dbc[item['symbol_id']] = [item]
    return dbc


def _get_color_statistics(csv_filepath='hasy-train-labels.csv', verbose=False):
    symbolid2latex = _get_symbolid2latex()
    data = _load_csv(csv_filepath)
    black_level, classes = [], []
    for symbol_id, elements in data_by_class(data).items():
        colors = _get_colors(elements)
        b = colors[0]
        w = colors[255]
        black_level.append(float(b) / (b + w))
        classes.append(symbol_id)
        if verbose:
            print("%s:\t%0.4f" % (symbol_id, black_level[-1]))
    print("Average black level: %0.4f" % numpy.average(black_level))
    print("Median black level: %0.4f" % numpy.median(black_level))
    print("Minimum black level: %0.4f (class: %s)" %
          (min(black_level),
           [symbolid2latex[c]
            for bl, c in zip(black_level, classes) if bl <= min(black_level)]))
    print("Maximum black level: %0.4f (class: %s)" %
          (max(black_level),
           [symbolid2latex[c]
            for bl, c in zip(black_level, classes) if bl >= max(black_level)]))


def _get_symbolid2latex(csv_filepath='symbols.csv'):
    symbol_data = _load_csv(csv_filepath)
    symbolid2latex = {}
    for row in symbol_data:
        symbolid2latex[row['symbol_id']] = row['latex']
    return symbolid2latex


def _analyze_class_distribution(dataset_path='.',
                                csv_filepath='hasy-train-labels.csv'):
    """Plot the distribution of training data over graphs."""
    symbol_id2index = generate_index(dataset_path)
    index2symbol_id = {}
    for index, symbol_id in symbol_id2index.items():
        index2symbol_id[symbol_id] = index
    data, y = load_images(dataset_path, csv_filepath, symbol_id2index,
                          one_hot=False)

    data = {}
    for el in y:
        if el in data:
            data[el] += 1
        else:
            data[el] = 1
    classes = data
    images = len(y)

    # Create plot
    print("Classes: %i" % len(classes))
    print("Images: %i" % images)

    class_counts = sorted([count for _, count in classes.items()])
    # plt.title('HASY training data distribution')
    plt.xlabel('Amount of available training images')
    plt.ylabel('Number of classes')
    min_examples = 0
    bin_size = 25
    MAX_DATA = 1000
    plt.hist(class_counts, bins=range(min_examples, MAX_DATA + 1, bin_size))
    # plt.show()
    filename = '{}.pdf'.format('training-data-dist')
    plt.savefig(filename)
    logging.info("Plot has been saved as {}".format(filename))

    symbolid2latex = _get_symbolid2latex()

    top10 = sorted(classes.items(), key=lambda n: n[1], reverse=True)[:10]
    top10_data = 0
    for index, count in top10:
        print("\t%s:\t%i" % (symbolid2latex[index2symbol_id[index]], count))
        top10_data += count
    total_data = sum([count for index, count in classes.items()])
    print("Top-10 has %i training data (%0.2f%% of total)" %
          (top10_data, float(top10_data) * 100.0 / total_data))
    print("%i classes have more than %i data items." %
          (sum([1 for _, count in classes.items() if count > 1000]), MAX_DATA))


def _analyze_pca(dataset_path='.', csv_filepath='hasy-train-labels.csv'):
    from sklearn.decomposition import PCA
    import itertools as it

    symbol_id2index = generate_index(dataset_path)
    data, y = load_images(dataset_path,
                          csv_filepath,
                          symbol_id2index,
                          one_hot=False)
    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])
    pca = PCA()
    pca.fit(data)
    sum_ = 0.0
    done_values = [None, None, None]
    done_points = [False, False, False]
    chck_points = [0.9, 0.95, 0.99]
    for counter, el in enumerate(pca.explained_variance_ratio_):
        sum_ += el
        for check_point, done, i in zip(chck_points, done_points, it.count()):
            if not done and sum_ >= check_point:
                done_points[i] = counter
                done_values[i] = sum_
    for components, variance in zip(done_points, done_values):
        print("%i components explain %0.2f of the variance" %
              (components, variance))


def _get_parser():
    """Get parser object for hasy_tools.py."""
    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    parser = ArgumentParser(description=__doc__,
                            formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument("--verify",
                        dest="verify",
                        action="store_true",
                        default=False,
                        help="verify PNG files")
    parser.add_argument("--overview",
                        dest="overview",
                        action="store_true",
                        default=False,
                        help="Get overview of data")
    parser.add_argument("--analyze_color",
                        dest="analyze_color",
                        action="store_true",
                        default=False,
                        help="Analyze the color distribution")
    parser.add_argument("--class_distribution",
                        dest="class_distribution",
                        action="store_true",
                        default=False,
                        help="Analyze the class distribution")
    parser.add_argument("--pca",
                        dest="pca",
                        action="store_true",
                        default=False,
                        help=("Show how many principal components explain "
                              "90%% / 95%% / 99%% of the variance"))
    return parser


if __name__ == "__main__":
    args = _get_parser().parse_args()
    if args.verify:
        _verify_all()
    if args.overview:
        img_src = _load_csv('hasy-train-labels.csv')
        create_random_overview(img_src, x_images=10, y_images=10)
    if args.analyze_color:
        _get_color_statistics(csv_filepath='hasy-train-labels.csv')
    if args.class_distribution:
        _analyze_class_distribution(csv_filepath='hasy-train-labels.csv')
    if args.pca:
        _analyze_pca(csv_filepath='hasy-train-labels.csv')
